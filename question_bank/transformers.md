## INTRO

1. Explain Self-attention mechanism in Transformers. What problem does it solve compared to traditional sequence models like RNNs and LSTMs?
2. Why do Transformers use positional encoding? How is it implemented mathematically?
3. What is multi-head attention, and why is it important in Transformers? How is it different from single-head attention?

## NORMALIZATION

4. Why do Transformers use Layer Normalization instead of Batch Normalization?
5. Why do Transformers use Residual Connections and Layer Normalization in each block?

## FFN
6. What is the role of the feed-forward network (FFN) in a Transformer block? Why do we need it after the self-attention layer?

## ENCODER DECODER

7. What are the key differences between the Encoder and Decoder in the Transformer model?

## ATTENTION
8. What is the difference between self-attention and cross-attention in a Transformer model?

## BEYOND TRANSFORMERS
20. Transformers use weight tying in language models. What does that mean, and why is it useful? 
